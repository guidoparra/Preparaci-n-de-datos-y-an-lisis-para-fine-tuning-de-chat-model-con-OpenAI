Procesamiento de Datos para Fine-Tuning de Modelos de Chat Este repositorio contiene un Jupyter Notebook (data_preparation.ipynb) dise√±ado para preparar y validar datasets en formato JSONL, espec√≠ficamente para fine-tuning de modelos de chat como los de OpenAI. El dataset utilizado abarca a la serie de televisi√≥n "Rick and Morty".

üìå Caracter√≠sticas principales Validaci√≥n de estructura: Verifica que los datos cumplan con el formato requerido para fine-tuning.

An√°lisis de tokens: Calcula la distribuci√≥n de tokens por mensaje y conversaci√≥n.

Estimaci√≥n de costos: Proporciona una proyecci√≥n del costo de entrenamiento basado en el volumen de tokens.

Detecci√≥n de problemas: Identifica ejemplos con formatos incorrectos o que exceden el l√≠mite de tokens.

üõ†Ô∏è Funcionalidades implementadas Carga y validaci√≥n de datos:

Verificaci√≥n de claves obligatorias (role, content).

Validaci√≥n de roles permitidos (system, user, assistant, function).

Detecci√≥n de mensajes faltantes (ej: ausencia de respuestas del asistente).

An√°lisis de tokens:

Conteo de tokens usando la codificaci√≥n cl100k_base de OpenAI.

Distribuci√≥n estad√≠stica (min/max, media/mediana, percentiles).

Identificaci√≥n de conversaciones que exceden el l√≠mite de 4096 tokens.

Estimaci√≥n de costos:

C√°lculo de tokens facturables.

Estimaci√≥n autom√°tica de √©pocas de entrenamiento.

Proyecci√≥n del costo total basado en el volumen de datos.

üìä Resultados destacables Dataset analizado: 1,261 ejemplos (todos v√°lidos).

Longitud promedio por ejemplo: 134 tokens (68-206 tokens).

Tokens de asistente promedio: 70 tokens por ejemplo.

0 ejemplos exceden el l√≠mite de tokens.

Costo estimado: ~506,466 tokens para 3 √©pocas de entrenamiento.
